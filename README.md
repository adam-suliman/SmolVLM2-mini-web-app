# Демо SmolVLM2 (Docker-first)

Этот проект содержит демонстрационный веб-интерфейс для моделей семейства **SmolVLM2** от Hugging Face, ориентированный на запуск в **Docker**.  
Приложение использует **Gradio** и позволяет:

1. **Мультимодальный чат (VQA / captioning / описание видео)**  
   - Загрузить **изображение** или **видео** (`.mp4`).  
   - Задать вопрос или запрос типа «Опиши изображение» / «Опиши видео».  
   - Задавать **несколько вопросов подряд** к одному и тому же файлу без повторной загрузки.

2. **OCR (распознавание текста) с выгрузкой результата**  
   - Загрузить изображение с текстом.  
   - Получить весь читаемый текст в виде **простого текста без комментариев**.  
   - Скачать результат как файл `ocr_result_<timestamp>.txt`.

3. **Описание объекта по координатам**  
   - Загрузить изображение.  
   - Ввести координаты прямоугольника в формате `x1,y1,x2,y2`.  
   - Получить описание объекта внутри заданной области.  
   - Формат координат строго валидируется (четыре числа, разделённые запятыми).

Все подписи и сообщения об ошибках в интерфейсе — на русском языке.

---

## Структура репозитория

- `app.py` — код приложения (Gradio + SmolVLM2 + три вкладки).  
- `requirements.txt` — список Python-зависимостей.  
- `Dockerfile` — описание Docker-образа.  
- `docker-compose.yml` — конфигурация сервиса для Docker Compose.  
- `.env.example` — пример файла с переменными окружения.  
- `data/hf_cache/` — директория на **хосте** для кэша моделей Hugging Face (создаётся вручную).

---

## Переменные окружения

Все важные параметры задаются через переменные окружения (записываются в `.env` или напрямую в `docker-compose.yml`):

- `SMOLVLM2_MODEL_ID` — идентификатор модели SmolVLM2 на Hugging Face, например:
  - `HuggingFaceTB/SmolVLM2-2.2B-Instruct` (по умолчанию, самая большая/тяжёлая);  
  - `HuggingFaceTB/SmolVLM2-500M-Video-Instruct`;  
  - `HuggingFaceTB/SmolVLM2-256M-Video-Instruct` (самая лёгкая).

- `MODEL_DEVICE` — выбор устройства для инференса:
  - `auto` — использовать GPU, если доступен, иначе CPU;  
  - `cuda` — принудительно попытаться использовать GPU (если GPU нет, будет предупреждение и падение обратно на CPU);  
  - `cpu` — принудительный запуск на CPU.

- `APP_PORT` — порт, на котором приложение слушает внутри контейнера и пробрасывается наружу (по умолчанию `7860`).

- `HF_HOME` — путь к директории кэша моделей внутри контейнера (по умолчанию `/data/hf_cache`).

- `HF_HUB_OFFLINE` — режим работы с Hugging Face Hub:
  - `0` — онлайн-режим (разрешён доступ к интернету для скачивания весов);  
  - `1` — офлайн-режим (используются только уже скачанные веса из кэша, без обращения к интернету).

Пример `.env` (показан здесь в виде текста, скопируйте в свой файл):

    SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-2.2B-Instruct
    MODEL_DEVICE=auto
    APP_PORT=7860
    HF_HOME=/data/hf_cache
    HF_HUB_OFFLINE=0

---

## Подготовка к запуску

1. Установите Docker и Docker Compose v2 (команда `docker compose`).  
2. Клонируйте репозиторий или скопируйте файлы проекта в отдельную директорию.  
3. В корне проекта создайте директорию для кэша моделей:

       mkdir -p data/hf_cache

4. Создайте файл `.env` на основе примера:

       cp .env.example .env

5. При необходимости измените в `.env` значения:
   - `SMOLVLM2_MODEL_ID`
   - `MODEL_DEVICE`
   - `APP_PORT`
   - `HF_HOME`
   - `HF_HUB_OFFLINE`

---

## Сборка Docker-образа

Сборка выполняется из корня репозитория (там, где находятся `Dockerfile` и `docker-compose.yml`):

    docker compose build

При этом:
- используется базовый образ `python:3.11-slim`;  
- устанавливаются системные пакеты (`ffmpeg`, `libgl1`, `git` и т.д.);  
- устанавливаются Python-зависимости из `requirements.txt`;  
- копируется `app.py`.

Важно: веса модели **не** скачиваются на этапе сборки — это делается при первом запуске контейнера (при наличии интернета).

---

## Монтирование директории с весами модели (кэш Hugging Face)

Чтобы веса не скачивались каждый раз заново, используется внешний том (фрагмент `docker-compose.yml`):

    volumes:
      - ./data/hf_cache:/data/hf_cache

- На **хосте** — каталог `./data/hf_cache`.  
- Внутри **контейнера** — каталог `/data/hf_cache` (значение `HF_HOME` по умолчанию).

Модель, токенизатор и другие файлы Hugging Face будут скачаны в `/data/hf_cache` внутри контейнера и автоматически окажутся в `./data/hf_cache` на хосте.

---

## Первый запуск (ОНЛАЙН, с загрузкой модели)

Эти шаги нужны, чтобы один раз скачать веса модели в локальный кэш.

1. Убедитесь, что в `.env`:

       HF_HUB_OFFLINE=0

2. Соберите образ (если ещё не):

       docker compose build

3. Запустите приложение:

       docker compose up

   При этом:
   - будет использована модель `SMOLVLM2_MODEL_ID` из `.env` (по умолчанию `HuggingFaceTB/SmolVLM2-2.2B-Instruct`);  
   - модель и связанные файлы скачаются в `./data/hf_cache`;  
   - приложение запустится на порту `APP_PORT` (по умолчанию `7860`).

4. Откройте браузер и перейдите по адресу:

       http://localhost:7860

   Если вы меняли `APP_PORT` (например, на `8080`), адрес будет:

       http://localhost:8080

5. Для остановки контейнера:
   - если вы запускали через `docker compose up` в foreground — `Ctrl+C`;  
   - либо выполните:

       docker compose down

---

## Повторный запуск и офлайн-режим

После первого онлайн-запуска все необходимые веса и файлы модели находятся в `./data/hf_cache`.

### Обычный повторный запуск (может ходить в интернет)

1. Убедитесь, что в `.env`:

       HF_HUB_OFFLINE=0

2. Запустите:

       docker compose up

### Полностью офлайн-запуск

Используйте только после того, как модель уже была один раз скачана.

1. Убедитесь, что каталог `data/hf_cache` содержит веса (предыдущий запуск онлайн был успешным).  
2. В `.env` установите:

       HF_HUB_OFFLINE=1

3. Запустите:

       docker compose up

В этом режиме библиотека `transformers` будет загружать модель только из локального кэша (`HF_HOME=/data/hf_cache`). Если необходимых файлов в кэше нет, загрузка модели завершится ошибкой.

---

## Адрес и порт доступа из хоста

В `docker-compose.yml` указано:

    ports:
      - "${APP_PORT:-7860}:${APP_PORT:-7860}"

Это значит:

- Внутри контейнера Gradio слушает на порту `APP_PORT` (по умолчанию 7860).  
- Этот же порт пробрасывается на хост как `localhost:APP_PORT`.

### Как поменять порт

1. Откройте `.env` и измените значение:

       APP_PORT=8080

2. Перезапустите сервис:

       docker compose down
       docker compose up

3. Теперь приложение доступно по адресу:

       http://localhost:8080

---

## Конфигурация выполнения на GPU / CPU

Выбор устройства управляется переменной `MODEL_DEVICE`:

- `auto` (по умолчанию):  
  - если внутри контейнера доступен GPU (и Docker запускался с поддержкой GPU), модель будет загружена на GPU;  
  - если GPU нет, будет использован CPU.

- `cuda`:  
  - приложение пытается использовать GPU;  
  - если GPU недоступен, в логах будет предупреждение, и выполнение переключится на CPU (fallback в коде).

- `cpu`:  
  - модель всегда выполняется на CPU, даже если GPU доступен.

### Пример: запуск с GPU

1. Настройте окружение Docker с поддержкой GPU (например, установите `nvidia-container-toolkit` и убедитесь, что контейнеры могут использовать GPU).  
2. В `.env`:

       MODEL_DEVICE=cuda

3. Запустите:

       docker compose up

Модель будет работать на GPU (при его наличии).

### Пример: принудительный запуск на CPU

В `.env`:

    MODEL_DEVICE=cpu

Перезапустите:

    docker compose down
    docker compose up

---

## Смена размера модели (разные варианты SmolVLM2)

Модель задаётся переменной `SMOLVLM2_MODEL_ID`. Можно использовать разные размеры в зависимости от ресурсов:

- Крупная модель (лучшее качество, больше требований к памяти):

      SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-2.2B-Instruct

- Средний размер:

      SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-500M-Video-Instruct

- Лёгкая модель:

      SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-256M-Video-Instruct

Чтобы сменить модель:

1. Обновите значение `SMOLVLM2_MODEL_ID` в `.env`.  
2. Перезапустите контейнер:

       docker compose down
       docker compose up

При первом запуске с новой моделью её веса будут скачаны в `./data/hf_cache`. В кэше может храниться несколько моделей одновременно.

---

## Типичные сценарии запуска (команды)

### 1. Первый запуск (онлайн, модель по умолчанию, порт 7860)

Из корня проекта:

    mkdir -p data/hf_cache
    cp .env.example .env
    docker compose build
    docker compose up

Открыть в браузере:

    http://localhost:7860

---

### 2. Запуск с другой моделью и другим портом

Пример `.env`:

    SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-500M-Video-Instruct
    MODEL_DEVICE=auto
    APP_PORT=8080
    HF_HOME=/data/hf_cache
    HF_HUB_OFFLINE=0

Команды:

    docker compose down
    docker compose up

Приложение будет доступно по адресу:

    http://localhost:8080

---

### 3. Офлайн-запуск после предварительной загрузки весов

Предполагается, что модель уже скачана (`data/hf_cache` заполнен после предыдущего онлайн-запуска).

В `.env`:

    HF_HUB_OFFLINE=1

Команда:

    docker compose up

---

## (Опционально) Локальный запуск без Docker

Если нужно запустить всё напрямую (например, для разработки):

1. Установите Python 3.11.  
2. Создайте виртуальное окружение и активируйте его.  
3. Установите зависимости:

       pip install -r requirements.txt

4. Задайте переменные окружения (пример для bash):

       export SMOLVLM2_MODEL_ID=HuggingFaceTB/SmolVLM2-2.2B-Instruct
       export MODEL_DEVICE=auto
       export APP_PORT=7860
       export HF_HOME=./data/hf_cache

5. Создайте директорию кэша:

       mkdir -p data/hf_cache

6. Запустите приложение:

       python app.py

Интерфейс будет доступен по адресу:

    http://localhost:7860
